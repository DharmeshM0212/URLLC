URLLC-in-the-Loop for Industrial Control — TDMA + k-of-n FEC:

This project builds a compact, software-only URLLC (Ultra-Reliable Low-Latency Communication) stack for industrial “STOP” decisions, targeting ≥99.9% on-time delivery with P99 latency ≤20 ms while keeping bandwidth overhead as low as possible. A producer emits 20 Hz decisions; a scheduler either sends best-effort (subject to heavy-tailed queueing jitter) or via TDMA micro-slots (near-deterministic timing); channels inject delay and loss using either an i.i.d. model or a Gilbert–Elliott burst model; a k-of-n erasure coder (FEC) reconstructs a decision as soon as any k of n chunks arrive; and an optional dual-path mode splits chunks over two independent links. The code sweeps scheduler mode, path count, and (n,k) to automatically choose the lowest-overhead configuration that satisfies the SLOs, and an end-to-end demo drives the network with a lightweight DSP endpoint detector so the results map to real control behavior.
The process follows six logical steps. First, establish a baseline: best-effort scheduling over an i.i.d. channel without FEC to quantify natural tail latency. Second, swap in TDMA to replace random queueing with reserved micro-slots, collapsing the tail. Third, add k-of-n FEC so multiple chunks race the deadline and the receiver reconstructs on the first k arrivals. Fourth, make the channel realistic and hard by introducing Gilbert–Elliott bursty loss and (optionally) two independent paths for diversity. Fifth, run an SLO-constrained sweep across scheduler/paths/(n,k) and select the minimal-overhead design that meets ≥99.9% on-time and P99 ≤20 ms, plotting P99-vs-overhead and a simple “overshoot-reduction” proxy. Finally, plug in a DSP endpoint detector (two-window mean + hysteresis on synthetic 1 kHz traces) so “STOP” decisions are produced by a real algorithm; measure detection delay plus network latency against a 50 ms end-to-end budget.
Results from my run are consistent with the theory. The i.i.d. baseline (best-effort, n=1,k=1) delivered ~98.17% on-time with P99 ≈18.24 ms—close to the 20 ms deadline. Switching to TDMA collapsed the tail (P99 ≈5.12 ms). Adding FEC on i.i.d. links reached near-perfect reliability: TDMA (n=3,k=1) achieved success=1.00 with P99 ≈4.66 ms at 3× overhead, while TDMA (n=4,k=2) hit success=1.00 with P99 ≈5.57 ms at 2× overhead. Under bursty Gilbert–Elliott loss, TDMA + FEC stayed robust, and dual-path offered extra insurance; the SLO sweep found a minimal-overhead winner TDMA, 1 path, (n=5,k=3) with success=1.00, P99 ≈6.89 ms at only 1.67× overhead. In the end-to-end demo with a 50 ms control budget, the baseline network achieved ~90% success, while a robust network (TDMA, 2 paths, n=4,k=2) achieved ~93.1%; the remaining gap was dominated by detector delay (P95 ≈25.8 ms), which can be reduced by modest parameter tuning. 
